{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP7sZGrepmuQ0dqN2xn6mfS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/showmik121/My_Python_code/blob/main/Genetive_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**sentimate analysis**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hTDF7B1-D71y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "pipe = pipeline(\"text-classification\")\n",
        "pipe([\"This restaurant is awesome\", \"This restaurant is awful\"])"
      ],
      "metadata": {
        "id": "8oaKfdrr72sX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "\n",
        "from transformers import pipeline\n",
        "pipe = pipeline(\"text-classification\")\n",
        "pipe([\"This restaurant is awesome\", \"This restaurant is awful\"])"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "6XoISFOtue5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optimum[onnxruntime]"
      ],
      "metadata": {
        "id": "6O4Zq6JAA52g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\"This restaurant is awesome\", \"This restaurant is awful\"]\n",
        "\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
        "\n",
        "model_id = \"SamLowe/roberta-base-go_emotions-onnx\"\n",
        "file_name = \"onnx/model_quantized.onnx\"\n",
        "\n",
        "model = ORTModelForSequenceClassification.from_pretrained(model_id, file_name=file_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "onnx_classifier = pipeline(\n",
        "    task=\"text-classification\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    top_k=None,\n",
        "    function_to_apply=\"sigmoid\",  # optional as is the default for the task\n",
        ")\n",
        "\n",
        "model_outputs = onnx_classifier(sentences)\n",
        "\n",
        "print(model_outputs)"
      ],
      "metadata": {
        "id": "mB0LoYfvAlf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# test generation"
      ],
      "metadata": {
        "id": "LKTNuCVCFDYY"
      }
    },
    {
      "source": [
        "from transformers import pipeline\n",
        "generator = pipeline('text-generation', model='EleutherAI/gpt-neo-125M')\n",
        "generator(\"Dhaka is a capital of Bangladesh.\", do_sample=True, min_length=50)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "OVSQH2iNaqMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline,set_seed\n",
        "generator = pipeline('text-generation', model='distilgpt2')\n",
        "set_seed(42)\n",
        "generator(\"Dhaka is capital of Bangladesh.\", truncation=True, num_return_sequences=2)\n"
      ],
      "metadata": {
        "id": "SVldxWPNd02-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# question answering"
      ],
      "metadata": {
        "id": "PGPw1YdTpQNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "nlp = pipeline(\"question-answering\")\n",
        "question=\"Where do I live?\"\n",
        "context=\"My name is Merve and I live in İstanbul.\"\n",
        "nlp(question = question, context = context)\n"
      ],
      "metadata": {
        "id": "j8NEa_EfpPOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer"
      ],
      "metadata": {
        "id": "4IbUOOm2drlQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification,DistilBertTokenizer,DistilBertForSequenceClassification\n"
      ],
      "metadata": {
        "id": "0L-AqtuNd3Bf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "text=\"Hello, how are you?\"\n",
        "tokens=tokenizer.tokenize(text)\n",
        "token_ids=tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(\"Tokens: \",tokens)\n",
        "print(\"Token IDs: \",token_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maQ5GPOWf3CV",
        "outputId": "fa6fb9da-ef31-4a37-de60-3d7970e983b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:  ['hello', ',', 'how', 'are', 'you', '?']\n",
            "Token IDs:  [7592, 1010, 2129, 2024, 2017, 1029]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaTokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "text=\"Hello, how are you?\"\n",
        "tokens=tokenizer.tokenize(text)\n",
        "token_ids=tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(\"Tokens: \",tokens)\n",
        "print(\"Token IDs: \",token_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGOB1AA_iVBa",
        "outputId": "b0ff2d10-990d-41b7-b52d-2a146dfb6ce7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:  ['Hello', ',', 'Ġhow', 'Ġare', 'Ġyou', '?']\n",
            "Token IDs:  [31414, 6, 141, 32, 47, 116]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "text=\"Hello, how are you?\"\n",
        "tokens=tokenizer.tokenize(text)\n",
        "token_ids=tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(\"Tokens: \",tokens)\n",
        "print(\"Token IDs: \",token_ids)\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "encoded_inputs=tokenizer([\"hello, how are you?,I am fine,thank you.\"],\n",
        "                         padding=True,\n",
        "                         truncation=True,\n",
        "                         return_tensors='pt')\n",
        "print(\"Encoder inputs: \", encoded_inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yx0H_ik4jr7A",
        "outputId": "ce3e9784-8eac-4e34-85a3-55e3bc24b90f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:  ['Hello', ',', 'Ġhow', 'Ġare', 'Ġyou', '?']\n",
            "Token IDs:  [15496, 11, 703, 389, 345, 30]\n",
            "Encoder inputs:  {'input_ids': tensor([[ 101, 7592, 1010, 2129, 2024, 2017, 1029, 1010, 1045, 2572, 2986, 1010,\n",
            "         4067, 2017, 1012,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pickle import decode_long\n",
        "from transformers import BertTokenizer\n",
        "tokenizer=BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "tex1=(\"Hello!,how are you?\")\n",
        "tex2=(\"I am fine,thank you.\")\n",
        "encoded_inputs=tokenizer.encode_plus(tex1,tex2,\n",
        "                         padding='max_length',\n",
        "                          add_special_tokens=True,\n",
        "                          max_length=12,\n",
        "                         truncation=True,\n",
        "                         return_tensors='pt')\n",
        "print(\"Encoder input IDs: \", encoded_inputs['input_ids'])\n",
        "print(\"Token Type IDs: \",encoded_inputs['token_type_ids'])\n",
        "print(\"Encoder attention masks: \", encoded_inputs['attention_mask'])\n",
        "# decoded the inputs ids back\n",
        "decode=tokenizer.decode(encoded_inputs['input_ids'][0],skip_special_tokens=False)\n",
        "print(decode)"
      ],
      "metadata": {
        "id": "pt8DvC8DaSSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tuning"
      ],
      "metadata": {
        "id": "W3_Rb8qCS9Ny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "guUblO7HTWqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"imdb\")\n",
        "ds['train'][0]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "TJT97AW4T7sM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
      ],
      "metadata": {
        "id": "4qjjSyjzXVxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the dataset\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_ds = ds.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "9Dm8MQEcZJtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_ds"
      ],
      "metadata": {
        "id": "SplxmU8r1kE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from transformers import TrainingArguments\n",
        "training_args = TrainingArguments(output_dir=\"./results\", evaluation_strategy=\"epoch\",learning_rate=2e-5,\n",
        "                                  per_device_train_batch_size=16, # Changed 'pre_device_train_batch_size' to 'per_device_train_batch_size'\n",
        "                                  per_device_eval_batch_size=16,\n",
        "                                  num_train_epochs=1,\n",
        "                                  weight_decay=0.01)\n",
        "training_args"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "CarJ6hd23omi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification,Trainer\n",
        "# load the pre-trained model\n",
        "model=AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\",num_labels=2)"
      ],
      "metadata": {
        "id": "wZMulWDg4G9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_ds[\"train\"],\n",
        "    eval_dataset=tokenized_ds[\"test\"],\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "h6VM71dJ42m5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "yXLBh_T45gBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evelute the model\n",
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "T_amMWMgSxDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the mode\n",
        "model.save_pretrained(\"./fine-tuned-model\")\n",
        "tokenizer.save_pretrained(\"./fine-tuned-tokenizer\")"
      ],
      "metadata": {
        "id": "t3HTAWz5S6lY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load fine-tuned model\n",
        "classifier = pipeline(\"text-classification\", model=\"fine_tuned_model\", tokenizer=\"fine_tuned_tokenizer\")\n",
        "\n",
        "# Test on a new review\n",
        "review = \"The movie was absolutely fantastic! The characters were well-developed.\"\n",
        "result = classifier(review)\n",
        "\n",
        "print(result)  # Example: [{'label': 'LABEL_1', 'score': 0.9991}] (LABEL_1 = positive, LABEL_0 = negative)\n"
      ],
      "metadata": {
        "id": "eMqXilkZTpNh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}